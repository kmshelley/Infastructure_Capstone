{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NOTE\n",
    "# This notebook server was started with command below, in which case \"sc\" is a spark context \n",
    "\n",
    "IPYTHON_OPTS=\"notebook --certfile=~/cert/mycert.pem --keyfile ~/cert/mykey.key\" $SPARK_HOME/bin/pyspark --master spark://spark1:7077 --jars $SPARK_HOME/jars/elasticsearch-hadoop-2.2.0.jar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/notebooks\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#lat/lon grid class\n",
    "import sys\n",
    "sys.path.append('../Infrastructure_Capstone')\n",
    "import os\n",
    "import math\n",
    "from shapely.geometry import Polygon\n",
    "from pyproj import Proj\n",
    "from elasticsearch import Elasticsearch\n",
    "from elasticsearch import helpers\n",
    "from elasticsearch.client import indices\n",
    "#from dataStorage import upload_to_Elasticsearch\n",
    "import ConfigParser\n",
    "from pprint import pprint\n",
    "from copy import deepcopy\n",
    "import datetime as dt\n",
    "from dateutil.parser import parse\n",
    "\n",
    "#read in the config file\n",
    "#os.chdir('~/Infrastructure_Capstone')\n",
    "config = ConfigParser.ConfigParser()\n",
    "config.read('../Infrastructure_Capstone/config/capstone_config.ini')\n",
    "\n",
    "ES_url = config.get('ElasticSearch','host')\n",
    "ES_password = config.get('ElasticSearch','password')\n",
    "ES_username= config.get('ElasticSearch','username')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.context.SparkContext object at 0x7faf3bbe0ad0>\n"
     ]
    }
   ],
   "source": [
    "#print sc to see what it is\n",
    "print sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(u'2013-02-06T13:00:00_10306',\n",
       " {u'grid_day': 6,\n",
       "  u'grid_dayOfWeek': 3,\n",
       "  u'grid_fullDate': u'2013-02-06T12:00:00-06:00',\n",
       "  u'grid_hourOfDay': 13,\n",
       "  u'grid_id': u'2013-02-06T13:00:00_10306',\n",
       "  u'grid_isAccident': 0,\n",
       "  u'grid_month': 2,\n",
       "  u'grid_zipcode': 10306})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_rdd = sc.newAPIHadoopRDD(\n",
    "    inputFormatClass=\"org.elasticsearch.hadoop.mr.EsInputFormat\",\n",
    "    keyClass=\"org.apache.hadoop.io.NullWritable\", \n",
    "    valueClass=\"org.elasticsearch.hadoop.mr.LinkedMapWritable\", \n",
    "    conf={ \"es.resource\" : \"dataframe/rows\", \"es.nodes\" : ES_url, \n",
    "          \"es.net.http.auth.user\" : ES_username, \n",
    "          \"es.net.http.auth.pass\" : ES_password })\n",
    "\n",
    "grid_rdd.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "weather_rdd = sc.newAPIHadoopRDD(\n",
    "    inputFormatClass=\"org.elasticsearch.hadoop.mr.EsInputFormat\",\n",
    "    keyClass=\"org.apache.hadoop.io.NullWritable\", \n",
    "    valueClass=\"org.elasticsearch.hadoop.mr.LinkedMapWritable\", \n",
    "    conf={ \"es.resource\" : \"weather_observations/weather_observations\", \"es.nodes\" : ES_url, \n",
    "          \"es.net.http.auth.user\" : ES_username, \n",
    "          \"es.net.http.auth.pass\" : ES_password })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(u'00178_20160208_11', {u'AltimeterFlag': None, u'ValueForWindCharacterFlag': None, u'DewPointCelsius': -3.0, u'DateTime': u'2016-02-08T11:55:00', u'WindSpeed': 9.0, u'WetBulbFarenheit': 32.0, u'WetBulbFarenheitFlag': None, u'WeatherType': None, u'DewPointCelsiusFlag': None, u'PressureTendencyFlag': None, u'WetBulbCelsiusFlag': None, u'SeaLevelPressure': u'99999.0', u'RecordType': u'AA', u'SkyConditionFlag': None, u'VisibilityFlag': None, u'Visibility': 10.0, u'obs_id': u'00178_20160208_11', u'DewPointFarenheit': 27.0, u'Date': u'2016-02-08T00:00:00', u'Altimeter': 29.8, u'WetBulbCelsius': -0.0, u'SkyCondition': u'OVC060', u'RelativeHumidity': 73.0, u'DryBulbFarenheitFlag': None, u'DewPointFarenheitFlag': None, u'DryBulbFarenheit': 35.0, u'PressureChangeFlag': None, u'DryBulbCelsius': 1.7, u'DryBulbCelsiusFlag': None, u'Time': u'1900-01-01T11:55:00', u'StationPressure': 29.78, u'RelativeHumidityFlag': None, u'RecordTypeFlag': None, u'WindSpeedFlag': None, u'WeatherTypeFlag': None, u'SeaLevelPressureFlag': None, u'HourlyPrecipFlag': None, u'StationPressureFlag': None, u'HourlyPrecip': u'99999.0', u'WBAN': u'00178', u'StationType': u'0', u'ValueForWindCharacter': None, u'PressureChange': u'99999.0', u'WindDirection': u'030', u'WindDirectionFlag': None, u'PressureTendency': None})\n"
     ]
    }
   ],
   "source": [
    "#print first row in accidents\n",
    "print weather_rdd.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#Grid Development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "<type 'str'>\n",
      "{'grid_boundary': {'coordinates': [[[-74.25909, 40.477399],\n",
      "                                    [-74.24775366120572,\n",
      "                                     40.477572560152176],\n",
      "                                    [-74.24798024634923,\n",
      "                                     40.486229302288606],\n",
      "                                    [-74.2593180408524, 40.48605572013411],\n",
      "                                    [-74.25909, 40.477399]]],\n",
      "                   'type': 'polygon'},\n",
      " 'grid_center': [-74.25353548711237, 40.48181428632108],\n",
      " 'grid_id': '0_0_19700101_0000',\n",
      " 'grid_time': datetime.datetime(1970, 1, 1, 0, 0)}\n"
     ]
    }
   ],
   "source": [
    "#Lambert Conformal Conic projection centered on the polygon definition\n",
    "lcc = '+proj=lcc +lat_1=%s +lat_2=%s +lat_0=%s +lon_0=%s +preserve_units = True +ellps=clrk66' % (33,45,40.6795535,-74.0006865)\n",
    "with open('./Infrastructure_Capstone/flatDataFiles/NYC_polygon.json','r') as geo_file:\n",
    "    geo = geojson.load(geo_file)\n",
    "    geo_poly = geo['features'][0]\n",
    "    poly = list(geojson.utils.coords(geo_poly))\n",
    "    #print len(poly)\n",
    "\n",
    "    \n",
    "##LET'S START WITH JUST A FEW DAYS\n",
    "data_start = dt.datetime(2012,6,30,17,0)\n",
    "data_end = dt.datetime(2012,7,3,17,0)\n",
    "#data_end = dt.datetime(2016,2,5,16,0)\n",
    "time_next = data_start\n",
    "\n",
    "times = []\n",
    "#create a list of time range starts to iterate over\n",
    "while time_next <= data_end:\n",
    "    times.append(time_next)\n",
    "    time_next+=dt.timedelta(seconds=86400)\n",
    "print len(times)\n",
    "data_bounds = [-74.25909,40.477399,-73.700009,40.917577]\n",
    "\n",
    "##DEFINE THE GRID OBJECT, FILTER FOR NYC ONLY\n",
    "grid = SearchGrid(data_bounds,50,filter_poly=poly,proj_str=lcc)\n",
    "from pprint import pprint\n",
    "pprint(grid.temporal_grid().next())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "import json\n",
    "def jsonify(grid):\n",
    "    #convert json-type object to json string\n",
    "    new_grid = deepcopy(grid)\n",
    "    new_grid['grid_time'] = int((new_grid['grid_time'] - dt.datetime(1970,1,1,0,0,0)).total_seconds()*1000) #convert datetime to milliseconds since 1970\n",
    "    return json.dumps(new_grid)\n",
    "    #return new_grid\n",
    "    \n",
    "def pyspark_upload_grid_doc_to_es(grid,start,end):\n",
    "    #input: a SearchGrid, start and end times\n",
    "    #uploads an rdd of the grid to ES\n",
    "    import json\n",
    "    grid = grid.temporal_grid(start,end)\n",
    "    \n",
    "    grid_rdd = sc.parallelize(grid).map(lambda doc: (doc['grid_id'],jsonify(doc)))\n",
    "    #print grid_rdd.count()\n",
    "    #print grid_rdd.first()\n",
    "    grid_rdd.saveAsNewAPIHadoopFile(\n",
    "        path='-', \n",
    "        outputFormatClass=\"org.elasticsearch.hadoop.mr.EsOutputFormat\",\n",
    "        keyClass=\"org.apache.hadoop.io.NullWritable\", \n",
    "        valueClass=\"org.elasticsearch.hadoop.mr.LinkedMapWritable\", \n",
    "        conf={\"es.resource\" : \"nyc_grid/grid\", \n",
    "              \"es.input.json\": \"true\",\n",
    "              \"es.nodes\" : ES_url, \n",
    "              \"es.net.http.auth.user\" : ES_username, \n",
    "              \"es.net.http.auth.pass\" : ES_password })\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.saveAsNewAPIHadoopFile.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 46.0 failed 1 times, most recent failure: Lost task 1.0 in stage 46.0 (TID 113, localhost): org.elasticsearch.hadoop.rest.EsHadoopNoNodesLeftException: Connection error (check network and/or proxy settings)- all nodes failed; tried [[169.53.138.85:9200]] \n\tat org.elasticsearch.hadoop.rest.NetworkClient.execute(NetworkClient.java:142)\n\tat org.elasticsearch.hadoop.rest.RestClient.execute(RestClient.java:423)\n\tat org.elasticsearch.hadoop.rest.RestClient.execute(RestClient.java:415)\n\tat org.elasticsearch.hadoop.rest.RestClient.bulk(RestClient.java:145)\n\tat org.elasticsearch.hadoop.rest.RestRepository.tryFlush(RestRepository.java:225)\n\tat org.elasticsearch.hadoop.rest.RestRepository.flush(RestRepository.java:248)\n\tat org.elasticsearch.hadoop.rest.RestRepository.doWriteToIndex(RestRepository.java:201)\n\tat org.elasticsearch.hadoop.rest.RestRepository.writeToIndex(RestRepository.java:163)\n\tat org.elasticsearch.hadoop.mr.EsOutputFormat$EsRecordWriter.write(EsOutputFormat.java:151)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopDataset$1$$anonfun$12$$anonfun$apply$4.apply$mcV$sp(PairRDDFunctions.scala:1036)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopDataset$1$$anonfun$12$$anonfun$apply$4.apply(PairRDDFunctions.scala:1034)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopDataset$1$$anonfun$12$$anonfun$apply$4.apply(PairRDDFunctions.scala:1034)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1206)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1042)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1014)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:88)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1280)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1268)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1267)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1267)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:697)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:697)\n\tat scala.Option.foreach(Option.scala:236)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:697)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1493)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1455)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1444)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:567)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1813)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1826)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1903)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopDataset$1.apply$mcV$sp(PairRDDFunctions.scala:1055)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopDataset$1.apply(PairRDDFunctions.scala:998)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopDataset$1.apply(PairRDDFunctions.scala:998)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:147)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:108)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:306)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopDataset(PairRDDFunctions.scala:998)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopFile$2.apply$mcV$sp(PairRDDFunctions.scala:938)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopFile$2.apply(PairRDDFunctions.scala:930)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopFile$2.apply(PairRDDFunctions.scala:930)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:147)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:108)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:306)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:930)\n\tat org.apache.spark.api.python.PythonRDD$.saveAsNewAPIHadoopFile(PythonRDD.scala:750)\n\tat org.apache.spark.api.python.PythonRDD.saveAsNewAPIHadoopFile(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:497)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:379)\n\tat py4j.Gateway.invoke(Gateway.java:259)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:207)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: org.elasticsearch.hadoop.rest.EsHadoopNoNodesLeftException: Connection error (check network and/or proxy settings)- all nodes failed; tried [[169.53.138.85:9200]] \n\tat org.elasticsearch.hadoop.rest.NetworkClient.execute(NetworkClient.java:142)\n\tat org.elasticsearch.hadoop.rest.RestClient.execute(RestClient.java:423)\n\tat org.elasticsearch.hadoop.rest.RestClient.execute(RestClient.java:415)\n\tat org.elasticsearch.hadoop.rest.RestClient.bulk(RestClient.java:145)\n\tat org.elasticsearch.hadoop.rest.RestRepository.tryFlush(RestRepository.java:225)\n\tat org.elasticsearch.hadoop.rest.RestRepository.flush(RestRepository.java:248)\n\tat org.elasticsearch.hadoop.rest.RestRepository.doWriteToIndex(RestRepository.java:201)\n\tat org.elasticsearch.hadoop.rest.RestRepository.writeToIndex(RestRepository.java:163)\n\tat org.elasticsearch.hadoop.mr.EsOutputFormat$EsRecordWriter.write(EsOutputFormat.java:151)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopDataset$1$$anonfun$12$$anonfun$apply$4.apply$mcV$sp(PairRDDFunctions.scala:1036)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopDataset$1$$anonfun$12$$anonfun$apply$4.apply(PairRDDFunctions.scala:1034)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopDataset$1$$anonfun$12$$anonfun$apply$4.apply(PairRDDFunctions.scala:1034)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1206)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1042)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1014)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:88)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-56-c0ab5d2bda3c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#THIS KEEPS THROWING EXCEPTIONS -- NEED TO WORK ON IT\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtime\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtimes\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mpyspark_upload_grid_doc_to_es\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrid\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mdt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimedelta\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseconds\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m86400\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[1;34m\"Done!\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-55-3b151c53ee5f>\u001b[0m in \u001b[0;36mpyspark_upload_grid_doc_to_es\u001b[1;34m(grid, start, end)\u001b[0m\n\u001b[0;32m     26\u001b[0m               \u001b[1;34m\"es.nodes\"\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mES_url\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m               \u001b[1;34m\"es.net.http.auth.user\"\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mES_username\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m               \"es.net.http.auth.pass\" : ES_password })\n\u001b[0m\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/spark/python/pyspark/rdd.pyc\u001b[0m in \u001b[0;36msaveAsNewAPIHadoopFile\u001b[1;34m(self, path, outputFormatClass, keyClass, valueClass, keyConverter, valueConverter, conf)\u001b[0m\n\u001b[0;32m   1374\u001b[0m                                                        \u001b[0moutputFormatClass\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1375\u001b[0m                                                        \u001b[0mkeyClass\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalueClass\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1376\u001b[1;33m                                                        keyConverter, valueConverter, jconf)\n\u001b[0m\u001b[0;32m   1377\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1378\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0msaveAsHadoopDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeyConverter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalueConverter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    536\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    537\u001b[0m         return_value = get_return_value(answer, self.gateway_client,\n\u001b[1;32m--> 538\u001b[1;33m                 self.target_id, self.name)\n\u001b[0m\u001b[0;32m    539\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    540\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/spark/python/pyspark/sql/utils.pyc\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     34\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m             \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    298\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    299\u001b[0m                     \u001b[1;34m'An error occurred while calling {0}{1}{2}.\\n'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 300\u001b[1;33m                     format(target_id, '.', name), value)\n\u001b[0m\u001b[0;32m    301\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    302\u001b[0m                 raise Py4JError(\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.saveAsNewAPIHadoopFile.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 46.0 failed 1 times, most recent failure: Lost task 1.0 in stage 46.0 (TID 113, localhost): org.elasticsearch.hadoop.rest.EsHadoopNoNodesLeftException: Connection error (check network and/or proxy settings)- all nodes failed; tried [[169.53.138.85:9200]] \n\tat org.elasticsearch.hadoop.rest.NetworkClient.execute(NetworkClient.java:142)\n\tat org.elasticsearch.hadoop.rest.RestClient.execute(RestClient.java:423)\n\tat org.elasticsearch.hadoop.rest.RestClient.execute(RestClient.java:415)\n\tat org.elasticsearch.hadoop.rest.RestClient.bulk(RestClient.java:145)\n\tat org.elasticsearch.hadoop.rest.RestRepository.tryFlush(RestRepository.java:225)\n\tat org.elasticsearch.hadoop.rest.RestRepository.flush(RestRepository.java:248)\n\tat org.elasticsearch.hadoop.rest.RestRepository.doWriteToIndex(RestRepository.java:201)\n\tat org.elasticsearch.hadoop.rest.RestRepository.writeToIndex(RestRepository.java:163)\n\tat org.elasticsearch.hadoop.mr.EsOutputFormat$EsRecordWriter.write(EsOutputFormat.java:151)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopDataset$1$$anonfun$12$$anonfun$apply$4.apply$mcV$sp(PairRDDFunctions.scala:1036)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopDataset$1$$anonfun$12$$anonfun$apply$4.apply(PairRDDFunctions.scala:1034)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopDataset$1$$anonfun$12$$anonfun$apply$4.apply(PairRDDFunctions.scala:1034)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1206)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1042)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1014)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:88)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1280)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1268)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1267)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1267)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:697)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:697)\n\tat scala.Option.foreach(Option.scala:236)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:697)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1493)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1455)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1444)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:567)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1813)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1826)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1903)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopDataset$1.apply$mcV$sp(PairRDDFunctions.scala:1055)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopDataset$1.apply(PairRDDFunctions.scala:998)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopDataset$1.apply(PairRDDFunctions.scala:998)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:147)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:108)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:306)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopDataset(PairRDDFunctions.scala:998)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopFile$2.apply$mcV$sp(PairRDDFunctions.scala:938)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopFile$2.apply(PairRDDFunctions.scala:930)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopFile$2.apply(PairRDDFunctions.scala:930)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:147)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:108)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:306)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:930)\n\tat org.apache.spark.api.python.PythonRDD$.saveAsNewAPIHadoopFile(PythonRDD.scala:750)\n\tat org.apache.spark.api.python.PythonRDD.saveAsNewAPIHadoopFile(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:497)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:379)\n\tat py4j.Gateway.invoke(Gateway.java:259)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:207)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: org.elasticsearch.hadoop.rest.EsHadoopNoNodesLeftException: Connection error (check network and/or proxy settings)- all nodes failed; tried [[169.53.138.85:9200]] \n\tat org.elasticsearch.hadoop.rest.NetworkClient.execute(NetworkClient.java:142)\n\tat org.elasticsearch.hadoop.rest.RestClient.execute(RestClient.java:423)\n\tat org.elasticsearch.hadoop.rest.RestClient.execute(RestClient.java:415)\n\tat org.elasticsearch.hadoop.rest.RestClient.bulk(RestClient.java:145)\n\tat org.elasticsearch.hadoop.rest.RestRepository.tryFlush(RestRepository.java:225)\n\tat org.elasticsearch.hadoop.rest.RestRepository.flush(RestRepository.java:248)\n\tat org.elasticsearch.hadoop.rest.RestRepository.doWriteToIndex(RestRepository.java:201)\n\tat org.elasticsearch.hadoop.rest.RestRepository.writeToIndex(RestRepository.java:163)\n\tat org.elasticsearch.hadoop.mr.EsOutputFormat$EsRecordWriter.write(EsOutputFormat.java:151)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopDataset$1$$anonfun$12$$anonfun$apply$4.apply$mcV$sp(PairRDDFunctions.scala:1036)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopDataset$1$$anonfun$12$$anonfun$apply$4.apply(PairRDDFunctions.scala:1034)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopDataset$1$$anonfun$12$$anonfun$apply$4.apply(PairRDDFunctions.scala:1034)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1206)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1042)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1014)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:88)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "#THIS KEEPS THROWING EXCEPTIONS -- NEED TO WORK ON IT\n",
    "for time in times:\n",
    "    pyspark_upload_grid_doc_to_es(grid,time,time+dt.timedelta(seconds=86400))\n",
    "    \n",
    "print \"Done!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
