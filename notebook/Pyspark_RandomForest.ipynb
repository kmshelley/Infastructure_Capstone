{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Classifier with PySpark MLLib\n",
    "\n",
    "To run the notebook with a Spark Context (sc) enter: 'IPYTHON_OPTS=\"notebook --certfile=~/cert/mycert.pem --keyfile ~/cert/mykey.key\" $SPARK_HOME/bin/pyspark --master spark://spark1:7077 --jars $SPARK_HOME/jars/elasticsearch-hadoop-2.2.0.jar'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "169.53.138.84\n"
     ]
    }
   ],
   "source": [
    "#ML Lib libraries\n",
    "from pyspark.mllib.tree import RandomForest, RandomForestModel\n",
    "from pyspark.mllib.util import MLUtils\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.ml.feature import OneHotEncoder\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "#Python Libraries\n",
    "import sys\n",
    "sys.path.append('../Infrastructure_Capstone')\n",
    "import os\n",
    "import random\n",
    "\n",
    "from elasticsearch import Elasticsearch\n",
    "from elasticsearch import helpers\n",
    "from elasticsearch.client import indices\n",
    "\n",
    "import ConfigParser\n",
    "\n",
    "#read in the config file\n",
    "config = ConfigParser.ConfigParser()\n",
    "config.read('../Infrastructure_Capstone/config/capstone_config.ini')\n",
    "\n",
    "ES_url = config.get('ElasticSearch','host')\n",
    "ES_password = config.get('ElasticSearch','password')\n",
    "ES_username= config.get('ElasticSearch','username')\n",
    "\n",
    "print ES_url\n",
    "seed = random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Configuration for reading from and writing to Elasticsearch\n",
    "es_read_conf = { \n",
    "        \"es.resource\" : \"dataframe_plus_weather/rows\", \n",
    "        \"es.nodes\" : ES_url,\n",
    "        \"es.port\" : \"9200\",\n",
    "        \"es.net.http.auth.user\" : ES_username,\n",
    "        \"es.net.http.auth.pass\" : ES_password \n",
    "    }\n",
    "\n",
    "es_write_conf = {\n",
    "        \"es.resource\" : \"rf_output/results\",\n",
    "        \"es.nodes\" : ES_url,\n",
    "        \"es.port\" : \"9200\",\n",
    "        \"es.net.http.auth.user\" : ES_username, \n",
    "        \"es.net.http.auth.pass\" : ES_password\n",
    "        #\"es.mapping.id\" : \"grid_id\"\n",
    "    } "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for MapReduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Function to convert dataframe rows to Labeled Points (label, features)\n",
    "def getLabeledPoint(row): \n",
    "    #input: SparkSQL dataframe row element\n",
    "    #output: Spark LabeledPoint\n",
    "    \n",
    "    #zip code\n",
    "    zipcode = row.grid_zipcodeIdx\n",
    "    \n",
    "    #date time fields\n",
    "    dayOfMonth = long(row.grid_day)\n",
    "    dayOfWeek = long(row.grid_dayOfWeek)\n",
    "    hour = long(row.grid_hourOfDay)\n",
    "    month = long(row.grid_month)\n",
    "    \n",
    "    #weather fields\n",
    "    fog = row.weather_FogIdx\n",
    "    rain = row.weather_RainIdx\n",
    "    snow = row.weather_SnowHailIceIdx\n",
    "    \n",
    "    if float(row.weather_WetBulbFarenheit) <> 99999:\n",
    "        temp = row.weather_WetBulbFarenheit  \n",
    "    else:\n",
    "        temp = 70\n",
    "        \n",
    "    if float(row.weather_HourlyPrecip) <> 99999:\n",
    "        precip = row.weather_HourlyPrecip  \n",
    "    else:\n",
    "        precip = 0\n",
    "        \n",
    "    if float(row.weather_Visibility) <> 99999:\n",
    "        vis = row.weather_Visibility  \n",
    "    else:\n",
    "        vis = 10\n",
    "        \n",
    "    if float(row.weather_WindSpeed) <> 99999:\n",
    "        windspeed = row.weather_WindSpeed  \n",
    "    else:\n",
    "        windspeed = 0\n",
    "    \n",
    "    #truth label\n",
    "    label = long(row.grid_isAccident)    \n",
    "    \n",
    "    return LabeledPoint(label,[zipcode,dayOfMonth,dayOfWeek,hour,month,fog,rain,snow,temp,precip,vis,windspeed])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gather data from Elasticsearch grid index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'grid_day': 6,\n",
       " u'grid_dayOfWeek': 3,\n",
       " u'grid_fullDate': u'2013-02-06T12:00:00-06:00',\n",
       " u'grid_hourOfDay': 13,\n",
       " u'grid_id': u'2013-02-06T13:00:00_10306',\n",
       " u'grid_isAccident': 0,\n",
       " u'grid_month': 2,\n",
       " u'grid_zipcode': 10306,\n",
       " u'weather_Fog': 0,\n",
       " u'weather_HourlyPrecip': 99999.0,\n",
       " u'weather_Rain': 0,\n",
       " u'weather_SkyCondition': u'SCT042',\n",
       " u'weather_SnowHailIce': 0,\n",
       " u'weather_Visibility': 10.0,\n",
       " u'weather_WeatherType': None,\n",
       " u'weather_WetBulbFarenheit': 31.0,\n",
       " u'weather_WindSpeed': 8.0}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get RDD of the collisions grid\n",
    "grid_rdd = sc.newAPIHadoopRDD(\n",
    "    inputFormatClass=\"org.elasticsearch.hadoop.mr.EsInputFormat\",\n",
    "    keyClass=\"org.apache.hadoop.io.NullWritable\", \n",
    "    valueClass=\"org.elasticsearch.hadoop.mr.LinkedMapWritable\", \n",
    "    conf=es_read_conf).map(lambda row: row[1])\n",
    "\n",
    "grid_rdd.first()\n",
    "#grid_test = sc.parallelize(grid_rdd.take(100000)) #start with a small test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#get set of accident and non-accident records\n",
    "accidents = grid_rdd.filter(lambda row: row['grid_isAccident'] == 1)\n",
    "accN = accidents.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "no_accidents = grid_rdd.filter(lambda row: row['grid_isAccident'] == 0)\n",
    "noaccN = no_accidents.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "570174 out of 6238371 grid elements have an accident.\n"
     ]
    }
   ],
   "source": [
    "print \"%s out of %s grid elements have an accident.\" % (str(accN),str(noaccN))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Data\n",
    "We will now randomly sample from the accidents and non-accidents RDD's to get approximately 50-50 accidents and non-accidents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total accidents: 427088\n",
      "Total non-accidents: 427836\n"
     ]
    }
   ],
   "source": [
    "fraction = 0.75\n",
    "sub_acc = accidents.sample(withReplacement=False,fraction=fraction,seed=seed)\n",
    "sub_noacc = no_accidents.sample(withReplacement=False,fraction=fraction*accN/noaccN,seed=seed)\n",
    "\n",
    "print \"Total accidents: %s\" % str(sub_acc.count())\n",
    "print \"Total non-accidents: %s\" % str(sub_noacc.count())\n",
    "full_rdd = sub_acc.union(sub_noacc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/spark/python/pyspark/sql/context.py:234: UserWarning: Using RDD of dict to inferSchema is deprecated. Use pyspark.sql.Row instead\n",
      "  warnings.warn(\"Using RDD of dict to inferSchema is deprecated. \"\n"
     ]
    }
   ],
   "source": [
    "#create a dataframe for encoding categorical variables\n",
    "#df = sqlContext.createDataFrame(full_rdd) #complete dataset\n",
    "df = sqlContext.createDataFrame(full_rdd.sample(withReplacement=False,fraction=0.25,seed=seed)) #let's start with a 1/4 of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#define categorical indexers for the data\n",
    "zipIndexer =  StringIndexer(inputCol='grid_zipcode', outputCol='grid_zipcodeIdx')\n",
    "fogIndexer =  StringIndexer(inputCol='weather_Fog', outputCol='weather_FogIdx')\n",
    "rainIndexer =  StringIndexer(inputCol='weather_Rain', outputCol='weather_RainIdx')\n",
    "snowIndexer =  StringIndexer(inputCol='weather_SnowHailIce', outputCol='weather_SnowHailIceIdx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(grid_collision_counter=1, grid_day=7, grid_dayOfWeek=2, grid_fullDate=u'2015-07-07T21:00:00-05:00', grid_hourOfDay=22, grid_id=u'2015-07-07T22:00:00_10031', grid_isAccident=1, grid_month=7, grid_zipcode=10031, weather_Fog=0, weather_HourlyPrecip=99999.0, weather_Rain=0, weather_SkyCondition=u'SCT049 SCT090 SCT200', weather_SnowHailIce=0, weather_Visibility=10.0, weather_WeatherType=None, weather_WetBulbFarenheit=73.0, weather_WindSpeed=10.0, grid_zipcodeIdx=112.0, weather_FogIdx=0.0, weather_RainIdx=0.0, weather_SnowHailIceIdx=0.0)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#index the zip codes and weather categories\n",
    "model1 = zipIndexer.fit(df)\n",
    "td1 = model1.transform(df)\n",
    "\n",
    "model2 = fogIndexer.fit(td1)\n",
    "td2 = model2.transform(td1)\n",
    "\n",
    "model3 = rainIndexer.fit(td2)\n",
    "td3 = model3.transform(td2)\n",
    "\n",
    "model4 = snowIndexer.fit(td3)\n",
    "td4 = model4.transform(td3)\n",
    "\n",
    "td4.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "zipEncoder = OneHotEncoder(dropLast=False, inputCol=\"grid_zipcodeIdx\", outputCol=\"grid_zipcodeVec\")\n",
    "zipEncoded = zipEncoder.transform(td4)\n",
    "\n",
    "fogEncoder = OneHotEncoder(dropLast=False, inputCol=\"weather_FogIdx\", outputCol=\"weather_FogVec\")\n",
    "fogEncoded = fogEncoder.transform(zipEncoded)\n",
    "\n",
    "rainEncoder = OneHotEncoder(dropLast=False, inputCol=\"weather_RainIdx\", outputCol=\"grid_RainVec\")\n",
    "rainEncoded = rainEncoder.transform(fogEncoded)\n",
    "\n",
    "snowEncoder = OneHotEncoder(dropLast=False, inputCol=\"weather_SnowHailIceIdx\", outputCol=\"weather_SnowHailIceVec\")\n",
    "fullEncoded = snowEncoder.transform(rainEncoded)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Run the Model\n",
    "The following Random Forest code comes directly from the Spark MLLib programming guide: \n",
    "http://spark.apache.org/docs/latest/mllib-ensembles.html#random-forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[LabeledPoint(1.0, [2.0,10.0,2.0,9.0,11.0,0.0,1.0,0.0,53.0,0.0,10.0,9.0]),\n",
       " LabeledPoint(1.0, [2.0,27.0,4.0,21.0,3.0,0.0,0.0,0.0,32.0,0.0,10.0,15.0]),\n",
       " LabeledPoint(1.0, [22.0,8.0,3.0,8.0,1.0,0.0,0.0,0.0,7.0,0.0,10.0,14.0])]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#convert the dataframe to labeled points\n",
    "labeled_pts = fullEncoded.map(getLabeledPoint)\n",
    "labeled_pts.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error = 0.296343690017\n",
      "Learned classification forest model:\n",
      "TreeEnsembleModel classifier with 3 trees\n",
      "\n",
      "  Tree 0:\n",
      "    If (feature 4 <= 3.0)\n",
      "     If (feature 2 <= 6.0)\n",
      "      If (feature 0 <= 131.0)\n",
      "       If (feature 8 <= 39.0)\n",
      "        Predict: 1.0\n",
      "       Else (feature 8 > 39.0)\n",
      "        Predict: 1.0\n",
      "      Else (feature 0 > 131.0)\n",
      "       If (feature 1 <= 14.0)\n",
      "        Predict: 0.0\n",
      "       Else (feature 1 > 14.0)\n",
      "        Predict: 0.0\n",
      "     Else (feature 2 > 6.0)\n",
      "      If (feature 3 <= 10.0)\n",
      "       If (feature 11 <= 5.0)\n",
      "        Predict: 0.0\n",
      "       Else (feature 11 > 5.0)\n",
      "        Predict: 0.0\n",
      "      Else (feature 3 > 10.0)\n",
      "       If (feature 9 <= 0.2)\n",
      "        Predict: 0.0\n",
      "       Else (feature 9 > 0.2)\n",
      "        Predict: 1.0\n",
      "    Else (feature 4 > 3.0)\n",
      "     If (feature 0 <= 131.0)\n",
      "      If (feature 3 <= 7.0)\n",
      "       If (feature 3 <= 6.0)\n",
      "        Predict: 0.0\n",
      "       Else (feature 3 > 6.0)\n",
      "        Predict: 1.0\n",
      "      Else (feature 3 > 7.0)\n",
      "       If (feature 0 <= 55.0)\n",
      "        Predict: 1.0\n",
      "       Else (feature 0 > 55.0)\n",
      "        Predict: 1.0\n",
      "     Else (feature 0 > 131.0)\n",
      "      If (feature 11 <= 8.0)\n",
      "       If (feature 9 <= 0.12)\n",
      "        Predict: 0.0\n",
      "       Else (feature 9 > 0.12)\n",
      "        Predict: 0.0\n",
      "      Else (feature 11 > 8.0)\n",
      "       If (feature 0 <= 166.0)\n",
      "        Predict: 0.0\n",
      "       Else (feature 0 > 166.0)\n",
      "        Predict: 0.0\n",
      "  Tree 1:\n",
      "    If (feature 3 <= 7.0)\n",
      "     If (feature 2 <= 5.0)\n",
      "      If (feature 3 <= 5.0)\n",
      "       If (feature 0 <= 131.0)\n",
      "        Predict: 0.0\n",
      "       Else (feature 0 > 131.0)\n",
      "        Predict: 0.0\n",
      "      Else (feature 3 > 5.0)\n",
      "       If (feature 3 <= 6.0)\n",
      "        Predict: 0.0\n",
      "       Else (feature 3 > 6.0)\n",
      "        Predict: 0.0\n",
      "     Else (feature 2 > 5.0)\n",
      "      If (feature 8 <= 56.0)\n",
      "       If (feature 0 <= 89.0)\n",
      "        Predict: 0.0\n",
      "       Else (feature 0 > 89.0)\n",
      "        Predict: 0.0\n",
      "      Else (feature 8 > 56.0)\n",
      "       If (feature 6 <= 0.0)\n",
      "        Predict: 0.0\n",
      "       Else (feature 6 > 0.0)\n",
      "        Predict: 0.0\n",
      "    Else (feature 3 > 7.0)\n",
      "     If (feature 3 <= 19.0)\n",
      "      If (feature 3 <= 12.0)\n",
      "       If (feature 0 <= 139.0)\n",
      "        Predict: 1.0\n",
      "       Else (feature 0 > 139.0)\n",
      "        Predict: 0.0\n",
      "      Else (feature 3 > 12.0)\n",
      "       If (feature 0 <= 139.0)\n",
      "        Predict: 1.0\n",
      "       Else (feature 0 > 139.0)\n",
      "        Predict: 0.0\n",
      "     Else (feature 3 > 19.0)\n",
      "      If (feature 5 <= 0.0)\n",
      "       If (feature 4 <= 2.0)\n",
      "        Predict: 0.0\n",
      "       Else (feature 4 > 2.0)\n",
      "        Predict: 0.0\n",
      "      Else (feature 5 > 0.0)\n",
      "       If (feature 11 <= 13.0)\n",
      "        Predict: 0.0\n",
      "       Else (feature 11 > 13.0)\n",
      "        Predict: 1.0\n",
      "  Tree 2:\n",
      "    If (feature 0 <= 139.0)\n",
      "     If (feature 3 <= 7.0)\n",
      "      If (feature 0 <= 44.0)\n",
      "       If (feature 4 <= 8.0)\n",
      "        Predict: 0.0\n",
      "       Else (feature 4 > 8.0)\n",
      "        Predict: 1.0\n",
      "      Else (feature 0 > 44.0)\n",
      "       If (feature 9 <= 0.01)\n",
      "        Predict: 0.0\n",
      "       Else (feature 9 > 0.01)\n",
      "        Predict: 0.0\n",
      "     Else (feature 3 > 7.0)\n",
      "      If (feature 8 <= 54.0)\n",
      "       If (feature 0 <= 77.0)\n",
      "        Predict: 1.0\n",
      "       Else (feature 0 > 77.0)\n",
      "        Predict: 1.0\n",
      "      Else (feature 8 > 54.0)\n",
      "       If (feature 11 <= 15.0)\n",
      "        Predict: 1.0\n",
      "       Else (feature 11 > 15.0)\n",
      "        Predict: 1.0\n",
      "    Else (feature 0 > 139.0)\n",
      "     If (feature 11 <= 7.0)\n",
      "      If (feature 1 <= 6.0)\n",
      "       If (feature 3 <= 7.0)\n",
      "        Predict: 0.0\n",
      "       Else (feature 3 > 7.0)\n",
      "        Predict: 0.0\n",
      "      Else (feature 1 > 6.0)\n",
      "       If (feature 8 <= 58.0)\n",
      "        Predict: 0.0\n",
      "       Else (feature 8 > 58.0)\n",
      "        Predict: 0.0\n",
      "     Else (feature 11 > 7.0)\n",
      "      If (feature 0 <= 166.0)\n",
      "       If (feature 11 <= 21.0)\n",
      "        Predict: 0.0\n",
      "       Else (feature 11 > 21.0)\n",
      "        Predict: 0.0\n",
      "      Else (feature 0 > 166.0)\n",
      "       If (feature 4 <= 1.0)\n",
      "        Predict: 0.0\n",
      "       Else (feature 4 > 1.0)\n",
      "        Predict: 0.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Split the data into training and test sets (30% held out for testing)\n",
    "(trainingData, testData) = labeled_pts.randomSplit([0.7, 0.3])\n",
    "\n",
    "# Train a RandomForest model.\n",
    "#  Empty categoricalFeaturesInfo indicates all features are continuous.\n",
    "#  Note: Use larger numTrees in practice.\n",
    "#  Setting featureSubsetStrategy=\"auto\" lets the algorithm choose.\n",
    "model = RandomForest.trainClassifier(trainingData, numClasses=2, categoricalFeaturesInfo={},\n",
    "                                     numTrees=3, featureSubsetStrategy=\"auto\",\n",
    "                                     impurity='gini', maxDepth=4, maxBins=32)\n",
    "\n",
    "# Evaluate model on test instances and compute test error\n",
    "predictions = model.predict(testData.map(lambda x: x.features))\n",
    "labelsAndPredictions = testData.map(lambda lp: lp.label).zip(predictions)\n",
    "testErr = labelsAndPredictions.filter(lambda (v, p): v != p).count() / float(testData.count())\n",
    "print('Test Error = ' + str(testErr))\n",
    "print('Learned classification forest model:')\n",
    "print(model.toDebugString())\n",
    "\n",
    "# Save and load model\n",
    "#model.save(sc, \"target/tmp/myRandomForestClassificationModel\")\n",
    "#sameModel = RandomForestModel.load(sc, \"target/tmp/myRandomForestClassificationModel\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "updated_rdd.saveAsNewAPIHadoopFile(\n",
    "            path='-', \n",
    "            outputFormatClass=\"org.elasticsearch.hadoop.mr.EsOutputFormat\",\n",
    "            keyClass=\"org.apache.hadoop.io.NullWritable\", \n",
    "            valueClass=\"org.elasticsearch.hadoop.mr.LinkedMapWritable\", \n",
    "            conf=es_write_conf)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra Stuff"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
