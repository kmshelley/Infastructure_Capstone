{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "from elasticsearch import helpers\n",
    "import ConfigParser\n",
    "import os\n",
    "\n",
    "#read in the config file\n",
    "config = ConfigParser.ConfigParser()\n",
    "\n",
    "#print os.path.isfile() \n",
    "config.read(os.getcwd() + \"/Infra_2/capstone_config.ini\")\n",
    "\n",
    "ES_url = config.get('ElasticSearch','host')\n",
    "ES_password = config.get('ElasticSearch','password')\n",
    "ES_username= config.get('ElasticSearch','username')\n",
    "#es = Elasticsearch(timeout=30)\n",
    "#es = Elasticsearch(timeout=30, max_retries=10, retry_on_timeout=True)\n",
    "es = Elasticsearch(['http://' + ES_username + ':' + ES_password + '@' + ES_url + ':9200/'],timeout=60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting list of all of the zipcodes in NYC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "213\n",
      "10271,10278,10279,10280,10282,10803,10301,10302,10303,10304,10305,10306,10307,10308,10309,10310,10311,10312,10314,11351,11354,11355,11356,11357,11358,11359,11360,11361,11362,11363,11364,11365,11366,11367,11368,11369,11370,11371,11372,11373,11374,11375,11377,11378,11379,11385,11201,11411,11412,11413,11414,11415,11416,11417,11418,11419,11420,11421,11422,11423,11424,11425,11426,11427,11428,11429,11430,11432,11433,11434,11435,11436,11451,11040,10451,10452,10453,10454,10455,10456,10457,10458,10459,10460,10461,10462,10463,10464,10465,10466,10467,10468,10469,10470,10471,10472,10473,10474,10475,11001,11004,11005,10001,10002,10003,10004,10005,10006,10007,10009,10010,10011,10012,10013,10014,10016,10017,10018,10019,10020,10021,10022,10023,10024,10025,10026,10027,10028,10029,10030,10031,10032,10033,10034,10035,10036,10037,10038,10039,10040,10044,10065,10069,10075,11101,11102,11103,11104,11105,11106,11109,10103,10110,10111,10112,10115,10119,10128,10152,10153,10154,11691,11692,11693,11694,11697,10162,10165,10167,10168,10169,10170,10171,10172,10173,10174,10177,11203,11204,11205,11206,11207,11208,11209,11210,11211,11212,11213,11214,11215,11216,11217,11218,11219,11220,11221,11222,11223,11224,11225,11226,11228,11229,11230,11231,11232,11233,11234,11235,11236,11237,11238,11239\n"
     ]
    }
   ],
   "source": [
    "collisions = helpers.scan(es,\n",
    "    query={\"query\": {\"match_all\": {}}},\n",
    "    index=\"nyc_zip_codes\",\n",
    "    doc_type=\"zip_codes\"\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "index = 0\n",
    "\n",
    "zipc=[]\n",
    "for collision in collisions:\n",
    "    #print collision\n",
    "    #print collision[\"_source\"][\"GEOID10\"]\n",
    "    #break\n",
    "    zipc.append(int(collision[\"_source\"][\"GEOID10\"]))    \n",
    "   \n",
    "    index +=1\n",
    "    \n",
    "#since we will get duplicates (we split each polygon in a multipolygon zipcode to one row each)\n",
    "#clean up duplicates\n",
    "uniqueZipFromZipCode =list(set(zipc))\n",
    "print len(uniqueZipFromZipCode)\n",
    "print \",\".join(map(str,uniqueZipFromZipCode))\n",
    "#print index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get all zip codes in collision data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "collisions = helpers.scan(es,\n",
    "    query={\"query\": {\"match_all\": {}}},\n",
    "    index=\"saferoad\",\n",
    "    doc_type=\"collisions\"\n",
    ")\n",
    "\n",
    "index = 0\n",
    "\n",
    "zipcFromCollisions=[]\n",
    "for collision in collisions:\n",
    "    #print collision\n",
    "    zc = collision[\"_source\"][\"collision_ZCTA_ZIP_NoSuffix\"]\n",
    "    if zc != \"NA\":        \n",
    "        zipcFromCollisions.append(int(collision[\"_source\"][\"collision_ZCTA_ZIP_NoSuffix\"]))    \n",
    "   \n",
    "    #index +=1\n",
    "    #if index == 20:\n",
    "    #   break\n",
    "    \n",
    "#clean up duplicates\n",
    "uniqueZipFromCollisions =list(set(zipcFromCollisions))\n",
    "#print len(uniqueZip)\n",
    "#print \",\".join(map(str,uniqueZip))\n",
    "##print index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for zc in uniqueZipFromCollisions:\n",
    "    if not zc in uniqueZipFromZipCode:\n",
    "        print zc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In collisions index, reindex zip code with column name without space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "indexName = \"saferoad\"\n",
    "docType = \"collisions\"\n",
    "\n",
    "collisions = helpers.scan(es,\n",
    "    query={\"query\": {\"match_all\": {}}},\n",
    "    index=indexName,\n",
    "    doc_type=docType\n",
    ")\n",
    "\n",
    "bulkIncrement = 1000\n",
    "\n",
    "bulkCounter= 0\n",
    "\n",
    "\n",
    "bulkPackage = []\n",
    "for sdf in collisions:\n",
    "    \n",
    "    zc = sdf[\"_source\"][\"collision_ZIP CODE\"]\n",
    "       \n",
    "    if len(zc) == 0:\n",
    "        zc = \"NA\"\n",
    "    else\n",
    "    \n",
    "    bulkPackage.append({\n",
    "        '_op_type': 'update',\n",
    "        '_index': indexName,\n",
    "        '_type': docType,\n",
    "        '_id': sdf[\"_id\"],\n",
    "        'doc': {'collision_ZIP_CODE_C': zc}\n",
    "    })\n",
    "    \n",
    "\n",
    "    #es.update(index='saferoad',doc_type='collisions',id=sdf[\"_id\"],\n",
    "    #            body={\"doc\": {\"DateTime\": concatDate(sdf[\"_source\"][\"Date\"],sdf[\"_source\"][\"Time\"])}})\n",
    " \n",
    "    \n",
    "    \n",
    "    \n",
    "    #bulkCounter +=1\n",
    "    #if bulkCounter == bulkIncrement: #update\n",
    "    #    helpers.bulk(es, bulkPackage)\n",
    "        #reset \n",
    "    #    bulkPackage = []\n",
    "    #    bulkCounter = 0\n",
    "        \n",
    "    \n",
    "        \n",
    "if len(bulkPackage) != 0:\n",
    "    helpers.bulk(es, bulkPackage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge collision data to the grid. \n",
    "### We are marking 1) count of accidents given zipcode & hour 2) accident (1) or not (0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "indexName = \"saferoad\"\n",
    "docType = \"collisions\"\n",
    "\n",
    "collisions = helpers.scan(es,\n",
    "    query={\"query\": {\"match_all\": {}}},\n",
    "    index=indexName,\n",
    "    doc_type=docType\n",
    ")\n",
    "\n",
    "#id in this format\n",
    "#2014-05-09T16:00:00_11005\n",
    "#2014-07-22T17:02:00\n",
    "\n",
    "#isProbZipcode = dict()\n",
    "#isProbZipcode[\"10281\"] = 0\n",
    "#isProbZipcode[\"10000\"] = 0\n",
    "#isProbZipcode[\"10048\"] = 0\n",
    "#isProbZipcode[\"11695\"] = 0\n",
    "#isProbZipcode[\"11249\"] = 0\n",
    "\n",
    "dateZiplist = dict()\n",
    "index = 0\n",
    "\n",
    "\n",
    "\n",
    "for col in collisions:\n",
    "    zipCode = col[\"_source\"][\"collision_ZCTA_ZIP_NoSuffix\"].encode('ascii','ignore')\n",
    "    if zipCode != \"NA\":\n",
    "        \n",
    "        dateTimeStr = col[\"_source\"][\"collision_DATETIME_C\"].encode('ascii','ignore')\n",
    "        #print dateTimeStr\n",
    "        accidentdt = datetime.datetime.strptime(dateTimeStr, '%Y-%m-%dT%H:%M:%S')        \n",
    "        hourMark = accidentdt.replace(minute=0, second=0)                \n",
    "        dateAndZip = hourMark.strftime('%Y-%m-%dT%H:%M:%S') + \"_\" + zipCode\n",
    "        #print dateAndZip\n",
    "        if dateAndZip in dateZiplist: #accident was already recorded at this location at this hour\n",
    "            #append to existing records\n",
    "            dateZiplist[dateAndZip][\"grid_collision_counter\"] += 1\n",
    "            #dateZiplist[dateAndZip][\"collisionRec_\" + col[\"_id\"]] =  1\n",
    "        else:\n",
    "            #insert new record\n",
    "            dateZiplist[dateAndZip] = dict()\n",
    "            dateZiplist[dateAndZip][\"grid_collision_counter\"] = 1\n",
    "            #dateZiplist[dateAndZip][\"grid_collisionRec_\" + col[\"_id\"]] =  1\n",
    "            dateZiplist[dateAndZip][\"grid_isAccident\"] =  1\n",
    "            \n",
    "\n",
    "    \n",
    "#helpers.bulk(es, bulkGridPackage,chunk_size=300)        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# based on the dict created above, put together bulk update package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#building bulk package \n",
    "\n",
    "bulkGridPackage = []        \n",
    "for key, value in dateZiplist.iteritems():\n",
    "    #print key\n",
    "    \n",
    "#    es.update(index='weather_observations',doc_type='weather_observations',id=sdf[\"_id\"],\n",
    "#        body={\"doc\": {\"DateTime\": concatDate(sdf[\"_source\"][\"Date\"],sdf[\"_source\"][\"Time\"])}})\n",
    "\n",
    "\n",
    "    bulkGridPackage.append({\n",
    "        '_op_type': 'update',\n",
    "        '_index': \"dataframe\",\n",
    "        '_type': \"rows\",\n",
    "        '_id': key,\n",
    "        'doc': value\n",
    "    })\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Couting how many times 5 zip codes in collision that never made it into master zip code list show up in collisions (OLD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zipcode:10048 count:29\n",
      "zipcode:11249 count:1955\n",
      "zipcode:11695 count:10\n",
      "zipcode:10000 count:7\n",
      "zipcode:10281 count:30\n"
     ]
    }
   ],
   "source": [
    "for key, value in isProbZipcode.iteritems():\n",
    "    print \"zipcode:\" + key + \" count:\" + str(value) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# checking to see if all of the accients exist in the grid\n",
    "### it seems an hour that was supposed to be \"lost\" when DST kicked in existed in the collision dataset (data entry error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2014-03-09T02:00:00_11203\n",
      "2014-03-09T02:00:00_11414\n",
      "2013-03-10T02:00:00_11225\n"
     ]
    }
   ],
   "source": [
    "index = 0\n",
    "for key, value in dateZiplist.iteritems():\n",
    "    #print key\n",
    "    #print value\n",
    "    index+=1\n",
    "    #break\n",
    "    row = es.search(index=\"dataframe\",doc_type='rows', size=1, \\\n",
    "                       body={\"query\": \\\n",
    "                                {\"match\": \\\n",
    "                                    {'_id':key} \\\n",
    "                                } \\\n",
    "                            })\n",
    "    \n",
    "    #print key\n",
    "    hits =  len(row[\"hits\"][\"hits\"])\n",
    "    #break  \n",
    "    if hits == 0:\n",
    "        print key\n",
    "\n",
    "    #if index == 1000:\n",
    "    #    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# insert collisions to the grid "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(570174,\n",
       " [{u'update': {u'_id': u'2013-03-10T02:00:00_11001',\n",
       "    u'_index': u'dataframe',\n",
       "    u'_type': u'rows',\n",
       "    u'error': {u'index': u'dataframe',\n",
       "     u'reason': u'[rows][2013-03-10T02:00:00_11001]: document missing',\n",
       "     u'shard': u'-1',\n",
       "     u'type': u'document_missing_exception'},\n",
       "    u'status': 404}},\n",
       "  {u'update': {u'_id': u'2014-03-09T02:00:00_11203',\n",
       "    u'_index': u'dataframe',\n",
       "    u'_type': u'rows',\n",
       "    u'error': {u'index': u'dataframe',\n",
       "     u'reason': u'[rows][2014-03-09T02:00:00_11203]: document missing',\n",
       "     u'shard': u'-1',\n",
       "     u'type': u'document_missing_exception'},\n",
       "    u'status': 404}},\n",
       "  {u'update': {u'_id': u'2014-03-09T02:00:00_11414',\n",
       "    u'_index': u'dataframe',\n",
       "    u'_type': u'rows',\n",
       "    u'error': {u'index': u'dataframe',\n",
       "     u'reason': u'[rows][2014-03-09T02:00:00_11414]: document missing',\n",
       "     u'shard': u'-1',\n",
       "     u'type': u'document_missing_exception'},\n",
       "    u'status': 404}},\n",
       "  {u'update': {u'_id': u'2013-03-10T02:00:00_11214',\n",
       "    u'_index': u'dataframe',\n",
       "    u'_type': u'rows',\n",
       "    u'error': {u'index': u'dataframe',\n",
       "     u'reason': u'[rows][2013-03-10T02:00:00_11214]: document missing',\n",
       "     u'shard': u'-1',\n",
       "     u'type': u'document_missing_exception'},\n",
       "    u'status': 404}},\n",
       "  {u'update': {u'_id': u'2013-03-10T02:00:00_11231',\n",
       "    u'_index': u'dataframe',\n",
       "    u'_type': u'rows',\n",
       "    u'error': {u'index': u'dataframe',\n",
       "     u'reason': u'[rows][2013-03-10T02:00:00_11231]: document missing',\n",
       "     u'shard': u'-1',\n",
       "     u'type': u'document_missing_exception'},\n",
       "    u'status': 404}},\n",
       "  {u'update': {u'_id': u'2013-03-10T02:00:00_11225',\n",
       "    u'_index': u'dataframe',\n",
       "    u'_type': u'rows',\n",
       "    u'error': {u'index': u'dataframe',\n",
       "     u'reason': u'[rows][2013-03-10T02:00:00_11225]: document missing',\n",
       "     u'shard': u'-1',\n",
       "     u'type': u'document_missing_exception'},\n",
       "    u'status': 404}}])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "helpers.bulk(es, bulkGridPackage,raise_on_error=False)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding month and day to the grid (NOTE: not used anymore b/c it was added in the grid creation stage)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "rows = helpers.scan(es,\n",
    "    query={\"query\": {\"match_all\": {}}},\n",
    "    index=\"dataframe\",\n",
    "    doc_type=\"rows\"\n",
    ")\n",
    "\n",
    "index = 0\n",
    "\n",
    "addMonthDayPackage=[]\n",
    "for row in rows:\n",
    "    #print row\n",
    "    #using _id row instead of grid_fullDate b/c grid_fullDate has timezone nightmare\n",
    "    thisDateTime =row[\"_id\"].split(\"_\")[0]\n",
    "\n",
    "    rowDt = datetime.datetime.strptime(thisDateTime, '%Y-%m-%dT%H:%M:%S') \n",
    "    #print rowDt.month\n",
    "    #print rowDt.day\n",
    "    \n",
    "    addMonthDayPackage.append({\n",
    "        '_op_type': 'update',\n",
    "        '_index': \"dataframe\",\n",
    "        '_type': \"rows\",\n",
    "        '_id': row[\"_id\"],\n",
    "        'doc': {'grid_month': rowDt.month, 'grid_day': rowDt.day}\n",
    "    })\n",
    " \n",
    "        \n",
    "    #zc = collision[\"_source\"][\"collision_ZIP_CODE_C\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6968370, [])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "helpers.bulk(es, addMonthDayPackage,raise_on_error=False)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove \"-\" from ZCTA zip codes and add as a new column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "collisions = helpers.scan(es,\n",
    "    query={\"query\": {\"match_all\": {}}},\n",
    "    index=\"saferoad\",\n",
    "    doc_type=\"collisions\"\n",
    ")\n",
    "\n",
    "index = 0\n",
    "\n",
    "zipcFromZCTA=[]\n",
    "for collision in collisions:\n",
    "    #print collision\n",
    "    zc = collision[\"_source\"][\"collision_ZCTA_ZIP\"]\n",
    "    \n",
    "    if not zc is None:        \n",
    "        if \"-\" in zc: #remove -1, -2 etc        \n",
    "            zc = zc.split(\"-\")[0]   \n",
    "            \n",
    "    else: #code None as NA as well\n",
    "        zc = \"NA\"\n",
    "\n",
    "    zipcFromZCTA.append({\n",
    "        '_op_type': 'update',\n",
    "        '_index': \"saferoad\",\n",
    "        '_type': \"collisions\",\n",
    "        '_id': collision[\"_id\"],\n",
    "        'doc': {'collision_ZCTA_ZIP_NoSuffix': zc }\n",
    "    })\n",
    "    \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(735535, [])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "helpers.bulk(es, zipcFromZCTA,raise_on_error=False)  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
