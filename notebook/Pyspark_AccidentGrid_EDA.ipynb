{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accident Grid Exploration with PySpark\n",
    "\n",
    "To run the notebook with a Spark Context (sc) enter: 'IPYTHON_OPTS=\"notebook --certfile=~/cert/mycert.pem --keyfile ~/cert/mykey.key\" $SPARK_HOME/bin/pyspark --master spark://spark1:7077 --jars $SPARK_HOME/jars/elasticsearch-hadoop-2.2.0.jar'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "169.53.138.84\n"
     ]
    }
   ],
   "source": [
    "#ML Lib libraries\n",
    "#Python Libraries\n",
    "import sys\n",
    "sys.path.append('../Infrastructure_Capstone')\n",
    "import os\n",
    "import random\n",
    "\n",
    "from elasticsearch import Elasticsearch\n",
    "from elasticsearch import helpers\n",
    "from elasticsearch.client import indices\n",
    "\n",
    "import ConfigParser\n",
    "\n",
    "#read in the config file\n",
    "config = ConfigParser.ConfigParser()\n",
    "config.read('../Infrastructure_Capstone/config/capstone_config.ini')\n",
    "\n",
    "ES_url = config.get('ElasticSearch','host')\n",
    "ES_password = config.get('ElasticSearch','password')\n",
    "ES_username= config.get('ElasticSearch','username')\n",
    "\n",
    "print ES_url\n",
    "seed = random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Configuration for reading from and writing to Elasticsearch\n",
    "es_read_conf = { \n",
    "        \"es.resource\" : \"dataframe_plus_weather/rows\", \n",
    "        \"es.nodes\" : ES_url,\n",
    "        \"es.port\" : \"9200\",\n",
    "        \"es.net.http.auth.user\" : ES_username,\n",
    "        \"es.net.http.auth.pass\" : ES_password \n",
    "    }\n",
    "\n",
    "es_write_conf = {\n",
    "        \"es.resource\" : \"rf_output/results\",\n",
    "        \"es.nodes\" : ES_url,\n",
    "        \"es.port\" : \"9200\",\n",
    "        \"es.net.http.auth.user\" : ES_username, \n",
    "        \"es.net.http.auth.pass\" : ES_password\n",
    "        #\"es.mapping.id\" : \"grid_id\"\n",
    "    } "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for MapReduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Function to convert ES rdd rows to Labeled Points (label, features)\n",
    "def getLabeledPoint(row): \n",
    "    #zip code\n",
    "    zipcode = row['grid_zipcodeIdx']\n",
    "    \n",
    "    #date time fields\n",
    "    dayOfMonth = long(row['grid_day'])\n",
    "    dayOfWeek = long(row['grid_dayOfWeek'])\n",
    "    hour = long(row['grid_hourOfDay'])\n",
    "    month = long(row['grid_month'])\n",
    "    \n",
    "    #weather fields\n",
    "    fog = row['weather_FogIdx']\n",
    "    rain = row['weather_RainIdx']\n",
    "    snow = row['weather_SnowHailIceIdx']\n",
    "    \n",
    "    if float(row['weather_WetBulbFarenheit']) <> 99999:\n",
    "        temp = row['weather_WetBulbFarenheit']  \n",
    "    else:\n",
    "        temp = 70\n",
    "        \n",
    "    if float(row['weather_HourlyPrecip']) <> 99999:\n",
    "        precip = row['weather_HourlyPrecip ']\n",
    "    else:\n",
    "        precip = 0\n",
    "        \n",
    "    if float(row['weather_Visibility']) <> 99999:\n",
    "        vis = row['weather_Visibility']\n",
    "    else:\n",
    "        vis = 10\n",
    "        \n",
    "    if float(row['weather_WindSpeed']) <> 99999:\n",
    "        windspeed = row['weather_WindSpeed']\n",
    "    else:\n",
    "        windspeed = 0\n",
    "    \n",
    "    #truth label\n",
    "    label = long(row['grid_isAccident'])   \n",
    "    \n",
    "    return LabeledPoint(label,[zipcode,dayOfMonth,dayOfWeek,hour,month,fog,rain,snow,temp,precip,vis,windspeed])\n",
    "\n",
    "#Function to convert dataframe rows to Labeled Points (label, features)\n",
    "def getLabeledPointDF(row): \n",
    "    #zip code\n",
    "    zipcode = row.grid_zipcodeIdx\n",
    "    \n",
    "    #date time fields\n",
    "    dayOfMonth = long(row.grid_day)\n",
    "    dayOfWeek = long(row.grid_dayOfWeek)\n",
    "    hour = long(row.grid_hourOfDay)\n",
    "    month = long(row.grid_month)\n",
    "    \n",
    "    #weather fields\n",
    "    fog = row.weather_FogIdx\n",
    "    rain = row.weather_RainIdx\n",
    "    snow = row.weather_SnowHailIceIdx\n",
    "    \n",
    "    if float(row.weather_WetBulbFarenheit) <> 99999:\n",
    "        temp = row.weather_WetBulbFarenheit  \n",
    "    else:\n",
    "        temp = 70\n",
    "        \n",
    "    if float(row.weather_HourlyPrecip) <> 99999:\n",
    "        precip = row.weather_HourlyPrecip  \n",
    "    else:\n",
    "        precip = 0\n",
    "        \n",
    "    if float(row.weather_Visibility) <> 99999:\n",
    "        vis = row.weather_Visibility  \n",
    "    else:\n",
    "        vis = 10\n",
    "        \n",
    "    if float(row.weather_WindSpeed) <> 99999:\n",
    "        windspeed = row.weather_WindSpeed  \n",
    "    else:\n",
    "        windspeed = 0\n",
    "    \n",
    "    #truth label\n",
    "    label = long(row.grid_isAccident)    \n",
    "    \n",
    "    return LabeledPoint(label,[zipcode,dayOfMonth,dayOfWeek,hour,month,fog,rain,snow,temp,precip,vis,windspeed])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gather data from Elasticsearch grid index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#get RDD of the collisions grid\n",
    "grid_rdd = sc.newAPIHadoopRDD(\n",
    "    inputFormatClass=\"org.elasticsearch.hadoop.mr.EsInputFormat\",\n",
    "    keyClass=\"org.apache.hadoop.io.NullWritable\", \n",
    "    valueClass=\"org.elasticsearch.hadoop.mr.LinkedMapWritable\", \n",
    "    conf=es_read_conf).map(lambda row: row[1])\n",
    "\n",
    "grid_rdd.first()\n",
    "grid_test = sc.parallelize(grid_rdd.take(100000)) #start with a small test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "570174"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get set of accident and non-accident records\n",
    "accidents = grid_rdd.filter(lambda row: row['grid_isAccident'] == 1)\n",
    "N = accidents.count()\n",
    "N\n",
    "#no_accidents = grid_test.filter(lambda row: row['grid_isAccident'] == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_rdd = sc.parallelize(grid_rdd.take(2000))\n",
    "test_df = sqlContext.createDataFrame(test_rdd.map(lambda row: row[1]),samplingRatio = 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#define categorical indexers for the data\n",
    "zipIndexer =  StringIndexer(inputCol='grid_zipcode', outputCol='grid_zipcodeIdx')\n",
    "fogIndexer =  StringIndexer(inputCol='weather_Fog', outputCol='weather_FogIdx')\n",
    "rainIndexer =  StringIndexer(inputCol='weather_Rain', outputCol='weather_RainIdx')\n",
    "snowIndexer =  StringIndexer(inputCol='weather_SnowHailIce', outputCol='weather_SnowHailIceIdx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#index the zip codes and weather categories\n",
    "model1 = zipIndexer.fit(test_df)\n",
    "td1 = model1.transform(test_df)\n",
    "\n",
    "model2 = fogIndexer.fit(td1)\n",
    "td2 = model2.transform(td1)\n",
    "\n",
    "model3 = rainIndexer.fit(td2)\n",
    "td3 = model3.transform(td2)\n",
    "\n",
    "model4 = snowIndexer.fit(td3)\n",
    "td4 = model4.transform(td3)\n",
    "\n",
    "td4.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Run the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error = 0.0\n",
      "Learned classification forest model:\n",
      "TreeEnsembleModel classifier with 3 trees\n",
      "\n",
      "  Tree 0:\n",
      "    Predict: 0.0\n",
      "  Tree 1:\n",
      "    Predict: 0.0\n",
      "  Tree 2:\n",
      "    Predict: 0.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#convert the dataframe to labeled points\n",
    "labeled_pts = td4.map(getLabeledPoint)\n",
    "\n",
    "# Split the data into training and test sets (30% held out for testing)\n",
    "(trainingData, testData) = labeled_pts.randomSplit([0.7, 0.3])\n",
    "\n",
    "# Train a RandomForest model.\n",
    "#  Empty categoricalFeaturesInfo indicates all features are continuous.\n",
    "#  Note: Use larger numTrees in practice.\n",
    "#  Setting featureSubsetStrategy=\"auto\" lets the algorithm choose.\n",
    "model = RandomForest.trainClassifier(trainingData, numClasses=2, categoricalFeaturesInfo={},\n",
    "                                     numTrees=3, featureSubsetStrategy=\"auto\",\n",
    "                                     impurity='gini', maxDepth=4, maxBins=32)\n",
    "\n",
    "# Evaluate model on test instances and compute test error\n",
    "predictions = model.predict(testData.map(lambda x: x.features))\n",
    "labelsAndPredictions = testData.map(lambda lp: lp.label).zip(predictions)\n",
    "testErr = labelsAndPredictions.filter(lambda (v, p): v != p).count() / float(testData.count())\n",
    "print('Test Error = ' + str(testErr))\n",
    "print('Learned classification forest model:')\n",
    "print(model.toDebugString())\n",
    "\n",
    "# Save and load model\n",
    "#model.save(sc, \"target/tmp/myRandomForestClassificationModel\")\n",
    "#sameModel = RandomForestModel.load(sc, \"target/tmp/myRandomForestClassificationModel\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "updated_rdd.saveAsNewAPIHadoopFile(\n",
    "            path='-', \n",
    "            outputFormatClass=\"org.elasticsearch.hadoop.mr.EsOutputFormat\",\n",
    "            keyClass=\"org.apache.hadoop.io.NullWritable\", \n",
    "            valueClass=\"org.elasticsearch.hadoop.mr.LinkedMapWritable\", \n",
    "            conf=es_write_conf)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
